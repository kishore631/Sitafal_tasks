{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS7FaWJlI8D7"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatGroq(temperature=0, groq_api_key=\"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\", model_name=\"llama-3.1-70b-versatile\")\n",
        "\n",
        "file_path = \"faiss_store_openai.pkl\"\n",
        "\n",
        "# Function to scrape content from a website\n",
        "def scrape_website(url):\n",
        "    # Send a request to the website\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract text from all paragraph tags (you can adjust this as needed)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text = ' '.join([para.get_text() for para in paragraphs])\n",
        "        return text\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the webpage: {url}\")\n",
        "        return \"\"\n",
        "\n",
        "# Process websites after input\n",
        "def process_websites(urls):\n",
        "    all_text = \"\"\n",
        "\n",
        "    # Scrape text from all websites\n",
        "    for url in urls:\n",
        "        print(f\"Processing website: {url}\")\n",
        "        extracted_text = scrape_website(url)\n",
        "        all_text += extracted_text + \"\\n\"\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    text_chunks = text_splitter.split_text(all_text)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vectorstore_openai = FAISS.from_texts(text_chunks, embeddings)\n",
        "\n",
        "    # Save FAISS index\n",
        "    print(\"Embedding Vector Started Building...✅✅✅\")\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Save the FAISS index to a pickle file\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        pickle.dump(vectorstore_openai, f)\n",
        "\n",
        "    print(\"Text extracted and FAISS index saved.\")\n",
        "\n",
        "# User input for URLs\n",
        "urls = input(\"Enter the website URLs (comma-separated): \").split(',')\n",
        "\n",
        "# Run processing after URL input\n",
        "process_websites(urls)\n",
        "\n",
        "# Query input\n",
        "query = input(\"Ask a Question: \")\n",
        "if query:\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            vectorstore = pickle.load(f)\n",
        "            chain = RetrievalQA.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n",
        "\n",
        "        # Get response\n",
        "        result = chain.run(query)\n",
        "\n",
        "        # Display answer\n",
        "        print(\"Answer:\")\n",
        "        print(result)\n"
      ]
    }
  ]
}