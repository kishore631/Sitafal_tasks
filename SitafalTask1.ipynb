{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L7rA6f-JW7_"
      },
      "outputs": [],
      "source": [
        "!pip install pdfminer.six\n",
        "!pip install streamlit\n",
        "!pip install pickle5\n",
        "!pip install langchain\n",
        "!pip install langchain-groq\n",
        "!pip install faiss-cpu\n",
        "!pip install huggingface_hub\n",
        "!pip install -U langchain-community\n",
        "from pdfminer.high_level import extract_text\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatGroq(temperature=0, groq_api_key=\"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\", model_name=\"llama-3.1-70b-versatile\")\n",
        "\n",
        "# File upload in Colab\n",
        "from google.colab import files\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "file_path = \"faiss_store_openai.pkl\"\n",
        "\n",
        "# Process PDFs after upload\n",
        "def process_pdfs():\n",
        "    all_text = \"\"\n",
        "\n",
        "    # Extract text from all PDFs\n",
        "    for uploaded_file in uploaded_files.keys():\n",
        "        extracted_text = extract_text(uploaded_file)\n",
        "        all_text += extracted_text + \"\\n\"\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    text_chunks = text_splitter.split_text(all_text)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vectorstore_openai = FAISS.from_texts(text_chunks, embeddings)\n",
        "\n",
        "    # Save FAISS index\n",
        "    print(\"Embedding Vector Started Building...✅✅✅\")\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Save the FAISS index to a pickle file\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        pickle.dump(vectorstore_openai, f)\n",
        "\n",
        "    print(\"Text extracted and FAISS index saved.\")\n",
        "\n",
        "# Run processing after file upload\n",
        "process_pdfs()\n",
        "\n",
        "# Query input\n",
        "query = input(\"Ask a Question: \")\n",
        "if query:\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            vectorstore = pickle.load(f)\n",
        "            chain = RetrievalQA.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n",
        "\n",
        "        # Get response\n",
        "        result = chain.run(query)\n",
        "\n",
        "        # Display answer\n",
        "        print(\"Answer:\")\n",
        "        print(result)\n"
      ]
    }
  ]
}